{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95226711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install Quandl\n",
    "!pip install investpy\n",
    "!pip install missingno\n",
    "!pip install pandas_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f12ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import quandl\n",
    "import investpy\n",
    "import pandas_ta as ta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43c400",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_array  = set()\n",
    "def link2df(URL,col_name,join_df,join=True,check_column=True,check_URL = True,clear_URL_array=False,show_details=False):\n",
    "    '''This function scraps the given link and returns dataframe\n",
    "    __________\n",
    "    Parameters:\n",
    "        URL(string): URL to be scrapped from bitcoin website\n",
    "        col_name(string): column name for dataframe\n",
    "        join_df(variable)= dataframe withwhich output dataframe will be left joined on Date\n",
    "        join(boolean)= iF True,join, else don't join\n",
    "        check_column(boolean)= check if column name already exists\n",
    "        check_URL(boolean)= check if URL is already processed\n",
    "        clear_URL_array(boolean)= if true URL_processed array will be cleared\n",
    "        show_details(boolean)= various details wil be printed such as scrapping first and last details, df head & df tail     \n",
    "        '''\n",
    "        \n",
    "    print(f'processing {col_name}')\n",
    "\n",
    "    #clear URL append array\n",
    "    if clear_URL_array==True:\n",
    "        URL_array.clear()\n",
    "\n",
    "    #set join parameters if false\n",
    "    if join == False:\n",
    "        join_df = None\n",
    "        check_column=False\n",
    "\n",
    "    #process column name by making it lowercase and replacing spaces,commas, full stops\n",
    "    col_name = col_name.lower().replace(',','').replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "    #col_name validation if exists already\n",
    "    if check_column==True and col_name in list(join_df.columns):\n",
    "        print(f'column {col_name} already esists in dataframe, stopped here')\n",
    "        return join_df\n",
    "\n",
    "    #URL validation if processes already\n",
    "    elif check_URL==True and URL in list(URL_array):\n",
    "        print(f'{URL} is already processed, stopped here')\n",
    "        return join_df \n",
    "\n",
    "    #web scrapping\n",
    "    page = requests.get(URL)\n",
    "    soup = page.content\n",
    "    soup = str(soup)\n",
    "    scraped_output = (soup.split('[[')[1]).split('{labels')[0][0:-2]\n",
    "    if show_details == True:\n",
    "        print('head')\n",
    "        print({scraped_output[0:30]})\n",
    "        print('tail')\n",
    "        print({scraped_output[-30:]})\n",
    "\n",
    "    processed_str = scraped_output.replace('new Date(','')\n",
    "    processed_str = processed_str.replace(')','')\n",
    "    processed_str = processed_str.replace('[','')\n",
    "    processed_str = processed_str.replace(']','')\n",
    "    processed_str = processed_str.replace('\"','')\n",
    "\n",
    "    processed_str_list = processed_str.split(',')\n",
    "    date_list,data_list = processed_str_list[::2],processed_str_list[1::2]\n",
    "\n",
    "    #validate column lengths\n",
    "    if len(date_list)!=len(data_list):\n",
    "        print(f'date & data length:{len(date_list),len(data_list),len(date_list)==len(data_list)}')\n",
    "\n",
    "    #convert list data to a dataframe\n",
    "    if join == False:\n",
    "        df = pd.DataFrame()\n",
    "        df['Date'] = pd.to_datetime(date_list)\n",
    "        df[col_name] = data_list\n",
    "        URL_array.add(URL)\n",
    "        if show_details == True:\n",
    "            print('*'*100)\n",
    "            print('df head')\n",
    "            print(df.head(1))\n",
    "            print('*'*100)\n",
    "            print('df tail')\n",
    "            print(df.tail(1))\n",
    "            print('*'*100)\n",
    "            print(f'df shape{df.shape}')\n",
    "            print('='*100)\n",
    "            \n",
    "        return df\n",
    "\n",
    "    elif col_name not in list(join_df.columns) and join == True:\n",
    "        df = pd.DataFrame()\n",
    "        df['Date'] = pd.to_datetime(date_list)\n",
    "        df[col_name] = data_list\n",
    "        join_df = pd.merge(join_df,df,on=['Date'],how='left')\n",
    "        URL_array.add(URL)\n",
    "        if show_details == True:\n",
    "            print('*'*100)\n",
    "            print('df head')\n",
    "            print(df.head(1))\n",
    "            print('*'*100)\n",
    "            print('df tail')\n",
    "            print(df.tail(1))\n",
    "            print('*'*100)\n",
    "            print(f'output df shape= {df.shape},joined_df shape = {join_df.shape}')\n",
    "            print('='*100)\n",
    "            print(f'Number of duplicate columns in dataframe {df.columns.duplicated().sum()}')\n",
    "            print('='*100)\n",
    "    \n",
    "        return join_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa14563",
   "metadata": {},
   "source": [
    "# Web scrapping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4a344",
   "metadata": {},
   "source": [
    "### Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b36d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = investpy.get_crypto_historical_data(crypto='bitcoin',from_date='01/04/2013',to_date='19/04/2021')\n",
    "final_df = final_df.reset_index()\n",
    "final_df.drop(['Currency','Volume'],inplace=True,axis=1)\n",
    "final_df.columns = ['Date','opening_price','highest_price','lowest_price','closing_price']\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe653ed1",
   "metadata": {},
   "source": [
    "### Number of transactions in blockchain per day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61899a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/bitcoin-transactions.html',\n",
    "                   'transactions in blockchain',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885ef58",
   "metadata": {},
   "source": [
    "### Average block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75878380",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/size-btc.html',\n",
    "                   'avg block size',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7b4a51",
   "metadata": {},
   "source": [
    "### Number of unique (from) adresses per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03549a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/sentbyaddress-btc.html',\n",
    "                   'sent by adress',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09388e1e",
   "metadata": {},
   "source": [
    "### Average mining difficulty per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/bitcoin-difficulty.html',\n",
    "                   'avg mining difficulty',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275b63e",
   "metadata": {},
   "source": [
    "### Average hashrate (hash/s) per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/bitcoin-hashrate.html',\n",
    "                   'avg hashrate',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f469b",
   "metadata": {},
   "source": [
    "### Mining Profitability USD/Day for 1 Hash/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862da1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/bitcoin-mining_profitability.html',\n",
    "                   'mining profitability',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00ae4a",
   "metadata": {},
   "source": [
    "### Sent coins in USD per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/sentinusd-btc.html',\n",
    "                   'Sent coins in USD',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc80d2",
   "metadata": {},
   "source": [
    "### Average transaction fee, USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/bitcoin-transactionfees.html',\n",
    "                   'avg transaction fees',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cdb17d",
   "metadata": {},
   "source": [
    "### Median transaction fee, USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fef717",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/bitcoin-median_transaction_fee.html',\n",
    "                   'median transaction fees',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a8141",
   "metadata": {},
   "source": [
    "### Average block time (minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a71a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/bitcoin-confirmationtime.html',\n",
    "                   'avg block time',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd7aa6",
   "metadata": {},
   "source": [
    "### Avg. Transaction Value, USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/transactionvalue-btc.html',\n",
    "                   'avg transaction value',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac11a2",
   "metadata": {},
   "source": [
    "### Median Transaction Value, USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f87862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/mediantransactionvalue-btc.html',\n",
    "                   'median transaction value',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55bf62",
   "metadata": {},
   "source": [
    "### Tweets per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a62e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/tweets-btc.html',\n",
    "                   'tweets',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aafd67",
   "metadata": {},
   "source": [
    "### Google Trends to \"Bitcoin\" @ 2012-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455826aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/google_trends-btc.html',\n",
    "                   'google trends',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845af969",
   "metadata": {},
   "source": [
    "### Number of unique (from or to) addresses per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb560d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/activeaddresses-btc.html',\n",
    "                   'active addresses',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0484b6c7",
   "metadata": {},
   "source": [
    "### Top 100 Richest Addresses to Total coins %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/top100cap-btc.html',\n",
    "                   'top100 to total percentage',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c6e55",
   "metadata": {},
   "source": [
    "### Average Fee Percentage in Total Block Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582328ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = link2df('https://bitinfocharts.com/comparison/fee_to_reward-btc.html',\n",
    "                   'avg fee to reward',join_df=final_df,join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759fd79",
   "metadata": {},
   "source": [
    "### Total number of bitcoins in circulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c2b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_in_circulation_df = quandl.get(\"BCHAIN/TOTBC\",authtoken='9ztFCcK4_e1xGo_gjzK7')\n",
    "btc_in_circulation_df = btc_in_circulation_df.rename(columns={'Value': 'number_of_coins_in_circulation'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7f59b",
   "metadata": {},
   "source": [
    "### Bitcoin Miners Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46cad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "miners_revenue_df = quandl.get(\"BCHAIN/MIREV\",authtoken='9ztFCcK4_e1xGo_gjzK7')\n",
    "miners_revenue_df = miners_revenue_df.rename(columns={'Value': 'miner_revenue'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d3b5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38441092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b723f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69484e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering data as we are considering this peiod only\n",
    "final_df = final_df[np.logical_and(final_df['Date'] >= '01/04/2013',final_df['Date'] <= '20/04/2020')].reset_index(drop=True)\n",
    "\n",
    "final_df = pd.merge(final_df,btc_in_circulation_df,on=['Date'],how='left')\n",
    "final_df = pd.merge(final_df,miners_revenue_df,on=['Date'],how='left')\n",
    "\n",
    "# Change the index to Date\n",
    "final_df = final_df.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d30c174",
   "metadata": {},
   "source": [
    "# Fill NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b306af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null value to nan\n",
    "final_df.replace(to_replace='null', value=np.nan,inplace=True)\n",
    "final_df.drop(final_df.tail(1).index,inplace=True)\n",
    "\n",
    "# Check NaN values\n",
    "print(f\"There is {final_df.isnull().values.sum()} NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef745c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_values = pd.DataFrame(final_df.isna().sum(),columns=['missing_count'])\n",
    "missing_values.sort_values(by='missing_count',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb47f8b",
   "metadata": {},
   "source": [
    "### Remove NaN values in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['tweets'].fillna(final_df['tweets'].rolling(40, min_periods=1).mean()).bfill().astype(float).plot(x=final_df.index.values,y='tweets',figsize=(25,5),grid=True)\n",
    "for i in list(final_df.loc[pd.isna(final_df['tweets']), :].index):\n",
    "    plt.axvline(x=i,color='r',alpha=0.1)\n",
    "plt.ylabel('n_tweets')\n",
    "plt.title('Date vs n_tweets(with highlighted imputation)')\n",
    "plt.show()\n",
    "final_df['tweets'] = final_df['tweets'].fillna(final_df['tweets'].rolling(40, min_periods=1).mean()).bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6581be",
   "metadata": {},
   "source": [
    "### Remove NaN values in active_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['active_addresses'].fillna(final_df['active_addresses'].rolling(40, min_periods=1).mean()).bfill().astype(float).plot(x=final_df.index.values,y='active_addresses',figsize=(25,5),grid=True)\n",
    "for i in list(final_df.loc[pd.isna(final_df['active_addresses']), :].index):\n",
    "    plt.axvline(x=i,color='r',alpha=0.1)\n",
    "plt.ylabel('active_addresses')\n",
    "plt.title('Date vs active_addresses(with highlighted imputation)')\n",
    "plt.show()\n",
    "final_df['active_addresses'] = final_df['active_addresses'].fillna(final_df['active_addresses'].rolling(40, min_periods=1).mean()).bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0894b326",
   "metadata": {},
   "source": [
    "### Remove NaN values in top100_to_total_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['top100_to_total_percentage'].fillna(final_df['top100_to_total_percentage'].rolling(40, min_periods=1).mean()).bfill().astype(float).plot(x=final_df.index.values,y='top100_to_total_percentage',figsize=(25,5),grid=True)\n",
    "for i in list(final_df.loc[pd.isna(final_df['top100_to_total_percentage']), :].index):\n",
    "    plt.axvline(x=i,color='r',alpha=0.1)\n",
    "plt.ylabel('top100_to_total_percentage')\n",
    "plt.title('Date vs top100_to_total_percentage(with highlighted imputation)')\n",
    "plt.show()\n",
    "final_df['top100_to_total_percentage'] = final_df['top100_to_total_percentage'].fillna(final_df['top100_to_total_percentage'].rolling(40, min_periods=1).mean()).bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee26ea8",
   "metadata": {},
   "source": [
    "### Remove NaN values in avg_block_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f56b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['avg_block_time'].fillna(final_df['avg_block_time'].rolling(40, min_periods=1).mean()).bfill().astype(float).plot(x=final_df.index.values,y='avg_block_time',figsize=(25,5),grid=True)\n",
    "for i in list(final_df.loc[pd.isna(final_df['avg_block_time']), :].index):\n",
    "    plt.axvline(x=i,color='r',alpha=0.1)\n",
    "plt.ylabel('avg_block_time')\n",
    "plt.title('Date vs avg_block_time(with highlighted imputation)')\n",
    "plt.show()\n",
    "final_df['avg_block_time'] = final_df['avg_block_time'].fillna(final_df['avg_block_time'].rolling(40, min_periods=1).mean()).bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402391ef",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "We fill the NaN values by using a rolling mean on 40 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bdbcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = pd.DataFrame(final_df.isna().sum(),columns=['missing_count'])\n",
    "missing_values.sort_values(by='missing_count',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['avg_hashrate'].fillna(final_df['avg_hashrate'].rolling(40, min_periods=1).mean()).bfill().astype(float).plot(x=final_df.index.values,y='avg_hashrate',figsize=(25,5),grid=True)\n",
    "\n",
    "\n",
    "plt.ylabel('miner_revenue')\n",
    "plt.title('Date vs miner_revenue(with highlighted imputation)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952adf31",
   "metadata": {},
   "source": [
    "# Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54511758",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2caaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05eb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_smoothening(df,feature_name,smoothening_type,smoothening_range=[7,30,90],show_plot=False,show_original_Feature_in_plot=True):\n",
    "    if smoothening_type == 'sma':\n",
    "        for j in smoothening_range:\n",
    "            df[f'{smoothening_type}{j} {feature_name}'] = ta.sma(df[feature_name],j) \n",
    "\n",
    "    elif smoothening_type == 'var':\n",
    "        for j in smoothening_range:\n",
    "            df[f'{smoothening_type}{j} {feature_name}'] = ta.variance(df[feature_name],j)\n",
    "\n",
    "    elif smoothening_type == 'stdev':\n",
    "        for j in smoothening_range:\n",
    "            df[f'{smoothening_type}{j} {feature_name}'] = ta.stdev(df[feature_name],j)\n",
    "    \n",
    "    elif smoothening_type == 'ema':\n",
    "        for j in smoothening_range:\n",
    "            df[f'{smoothening_type}{j} {feature_name}'] = ta.ema(df[feature_name],j)\n",
    "\n",
    "    elif smoothening_type == 'wma':\n",
    "        for j in smoothening_range:\n",
    "            df[f'{smoothening_type}{j} {feature_name}'] = ta.wma(df[feature_name],j)\n",
    "\n",
    "    elif smoothening_type == 'rsi':\n",
    "        for j in smoothening_range:\n",
    "             df[f'{smoothening_type}{j} {feature_name}'] = ta.rsi(df[feature_name],j)\n",
    "\n",
    "    elif smoothening_type == 'roc':\n",
    "        for j in smoothening_range:\n",
    "            df[f'{smoothening_type}{j} {feature_name}'] = ta.roc(df[feature_name],j)  \n",
    "\n",
    "    elif smoothening_type == 'dema':\n",
    "        for j in smoothening_range:\n",
    "            df[f'{smoothening_type}{j} {feature_name}'] = ta.dema(df[feature_name],j) \n",
    "\n",
    "    elif smoothening_type == 'tema':\n",
    "        for j in smoothening_range:\n",
    "            df[f'{smoothening_type}{j} {feature_name}'] = ta.tema(df[feature_name],j) \n",
    "\n",
    "    elif smoothening_type == 'bband_lower':\n",
    "        for j in smoothening_range:\n",
    "            bband_df = ta.bbands(df[feature_name],j)\n",
    "            df[f'{smoothening_type}{j} {feature_name}'] = bband_df[f'BBL_{j}_2.0']\n",
    "\n",
    "    elif smoothening_type == 'bband_upper':\n",
    "        for j in smoothening_range:\n",
    "            bband_df = ta.bbands(df[feature_name],j)\n",
    "            df[f'{smoothening_type}{j} {feature_name}'] = bband_df[f'BBU_{j}_2.0']\n",
    "\n",
    "    elif smoothening_type == 'macd':\n",
    "        macd_df = ta.macd(df[feature_name])\n",
    "        df[f'{smoothening_type} hist {feature_name}'] = macd_df['MACDh_12_26_9']\n",
    "        df[f'{smoothening_type} signal {feature_name}'] = macd_df['MACDs_12_26_9']\n",
    "        df[f'{smoothening_type} {feature_name}'] = macd_df['MACD_12_26_9']\n",
    "\n",
    "    \n",
    "    if show_plot == True and show_original_Feature_in_plot==True :\n",
    "        df[[feature_name]+[i for i in list(df.columns) if i.split(\" \")[-1] == feature_name and i.split(\" \")[0][0:len(smoothening_type)] == smoothening_type]].plot(kind='line',figsize=(25,5))\n",
    "        plt.grid()\n",
    "        plt.title(f'Feature Smoothening-{feature_name} by {smoothening_type}')\n",
    "        plt.xticks([])\n",
    "        plt.show()\n",
    "\n",
    "    elif show_plot == True and show_original_Feature_in_plot==False :\n",
    "        df[[i for i in list(df.columns) if i.split(\" \")[-1] == feature_name and i.split(\" \")[0][0:len(smoothening_type)] == smoothening_type]].plot(kind='line',figsize=(25,5))\n",
    "        plt.grid()\n",
    "        plt.title(f'Feature Smoothening-{feature_name} by {smoothening_type}')\n",
    "        plt.xticks([])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9bef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [i for i in list(final_df.columns) if i not in ['Date','next_day_closing_price']]\n",
    "for feature in feature_list:\n",
    "    feature_smoothening(final_df,feature,'sma',show_plot=True)\n",
    "    feature_smoothening(final_df,feature,'var',show_plot=True)\n",
    "    feature_smoothening(final_df,feature,'stdev',show_plot=True)\n",
    "    feature_smoothening(final_df,feature,'ema',show_plot=True)\n",
    "    feature_smoothening(final_df,feature,'wma',show_plot=True)\n",
    "    feature_smoothening(final_df,feature,'rsi',show_plot=True)\n",
    "    feature_smoothening(final_df,feature,'roc',show_plot=True)\n",
    "    feature_smoothening(final_df,feature,'dema',show_plot=True)\n",
    "    feature_smoothening(final_df,feature,'tema',show_plot=True)\n",
    "    feature_smoothening(final_df,feature,'bband_lower',show_plot=True)\n",
    "    feature_smoothening(final_df,feature,'bband_upper',show_plot=True)\n",
    "    feature_smoothening(final_df,feature,'macd',show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c2e6f",
   "metadata": {},
   "source": [
    "# Test of stationarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Statsmodels \n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF Test\n",
    "def adf_test(timeseries):\n",
    "    print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n",
    "# Call the function and run the test\n",
    "\n",
    "adf_test(final_df[\"closing_price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071081d",
   "metadata": {},
   "source": [
    "The Augmented Dickey Test (ADF test)is one of the most common statistical tests used to test the stationarity of a given Time Series. It's a unit root test. \n",
    "\n",
    "Here, we observe the same order of magnitude for the ADF statistic -1.697262 with a p-value p = 0.43. The Time Series has then a  unit root and a trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3639f92",
   "metadata": {},
   "source": [
    "# Data visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"closing_price\"].plot()\n",
    "plt.title(\"Bitcoin (BTC) prices from January 2012 to March 2021\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"BTC price in USD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f92651",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Daily Lag')\n",
    "pd.plotting.lag_plot(final_df[\"closing_price\"], lag = 1)\n",
    "plt.show()\n",
    "\n",
    "plt.title('Monthly Lag')\n",
    "pd.plotting.lag_plot(final_df[\"closing_price\"], lag = 30)\n",
    "plt.show()\n",
    "\n",
    "plt.title('Yearly Lag')\n",
    "pd.plotting.lag_plot(final_df[\"closing_price\"], lag = 365)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c10ce",
   "metadata": {},
   "source": [
    "Thanks to lag plots, one can observethe autocorrelation of the data. The data shows a correlation for daily lag plots. For monthly lag plots, it seems that there is no correlation around 2017 when the price of bitcoin skyrocketed.Concerning, the yearly lag plots, there is no correlation at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7860b07",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0887f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a column class as in the article \n",
    "# if P_btc[t+1]-P_btc[t] >=0 --> y[t] = +1 otherwise y[t]=0\n",
    "\n",
    "final_df[\"class\"] = (np.sign(final_df[\"closing_price\"] - final_df[\"closing_price\"].shift()) + 1)//2\n",
    "final_df[\"class\"][0] = 1 # otherwise, it's a NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = final_df.drop(\"class\",axis = 1)\n",
    "Y = final_df[\"class\"]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa212c",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "### Scrapping \n",
    "https://github.com/rohansawant7978/bitcoin-price-forecasting\n",
    "\n",
    "### Augmented Dickey-Fuller\n",
    "https://www.analyticsvidhya.com/blog/2021/06/statistical-tests-to-check-stationarity-in-time-series-part-1/#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model ???\n",
    "\n",
    "https://github.com/heliphix/btc_data\n",
    "\n",
    "a classer\n",
    "\n",
    "\n",
    "https://github.com/sergiovirahonda/AnomalyDetection/blob/main/Bitcoin/Notebook/forecasting-and-anomally-detection.ipynb\n",
    "\n",
    "\n",
    "https://github.com/jaskirat111/Bitcoin-Time-Series-Forecast-ML-System\n",
    "\n",
    "\n",
    "https://github.com/Pranjal-26/Time-series-Forecasting---TensorFlow/blob/main/Final_project%20(1).ipynb\n",
    "\n",
    "https://github.com/Ashishkutchi/capstone-w2020-t2-g03-Time-Series-Forecasting-of-Bitcoin-Price\n",
    "\n",
    "https://github.com/AmalVijayan/Time-Series-Forecasting---Predicting-Bitcoin-Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1129f11",
   "metadata": {},
   "source": [
    "# TO DO \n",
    "\n",
    "### Scrap data used by the article \n",
    "Check if everything is scrapped\n",
    "\n",
    "Remove NaN values\n",
    "### Preprocess data \n",
    "linear interpolation to have missings cases\n",
    "\n",
    "create train test data\n",
    "\n",
    "### Visualisation \n",
    "pca \n",
    "\n",
    "graphs from the article\n",
    "\n",
    "correlation between features\n",
    "\n",
    "### Improvement \n",
    "Test on new datas --> avg block time --> must take more time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab617c",
   "metadata": {},
   "source": [
    "# Remarks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee508cc",
   "metadata": {},
   "source": [
    "Generally, the missing values are the number of tweets --> difficult to approximate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
